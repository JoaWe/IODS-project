#**Clustering and Classification**#

*Clusters* are combined elements of the target population. In certain cases the individual elements cannot (easily) be identified, while lists of the combined elements might be available, and accordingly usable for research.
A problem that persists by using *cluster samples*, however, is the so called *cluster effect*. This term is used to describe the issue that a certain loss of estimation exactness must be taken into account, because clusters themselves may differ quite a lot from each other, while a cluster's individual elements may display a strong resemblence with one another - sometimes even more similar to one another than within a simple sample. The greater the difference between the clusters, while the similarity of their individual elements increases, the less exact is the clustersampling.

####Loading and exploring the data *Boston*:####


```{r loading and exploring "Boston"}


#activating libraries
library(tidyr)
library(corrplot)
library(ggplot2)
library(dplyr)

#access MASS-Package
library(MASS)

#load the data
data("Boston")

#exploring the data
dim(Boston)
str(Boston)
summary(Boston)

```

The dataset consists of 14 variables including 506 observations relating to the city of Boston.

Here is a list of the variables and their respective meanings:

- **crim:** per capita crime rate by town.
- **zn:** proportion of residential land zoned for lots over 25,000 sq.ft.
- **indus:** proportion of non-retail business acres per town.
- **chas:** Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- **nox:** nitrogen oxides concentration (parts per 10 million).
- **rm:** average number of rooms per dwelling.
- **age:** proportion of owner-occupied units built prior to 1940.
- **dis:** weighted mean of distances to five Boston employment centres.
- **rad:** index of accessibility to radial highways.
- **tax:** full-value property-tax rate per \$10,000.
- **ptratio:** pupil-teacher ratio by town.
- **black:** 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- **lstat:** lower status of the population (percent).
- **medv:** median value of owner-occupied homes in \$1000s.

*These definitions and further information can be looked up from [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).*

####Graphical Overview of the dataset:####




####Scaling the dataset "Boston"####

```{r standardizing "Boston"}

#center and standardize "Boston"
boston_scaled <- scale(Boston)

#summarizing boston scaled
summary(boston_scaled)

#changing the object to a dataframe
boston_scaled <- as.data.frame(boston_scaled)

```

####Creating Variable "crime"####

In the following steps I intend to create a new categorical variable, named "crime", and use it to replace the original variable "crim".
The quantiles are used as break points for this categorical variable. Finally the new dataset will be divided to train- and test-sets, so that 80% of the data belongs to the train set.

```{r creating variable "crime"}

#summary of scaled variable "crim"
summary(boston_scaled$crim)

#creating and printing the quantile vector of "crim"
bins <- quantile(boston_scaled$crim)

#printing quantile vector
bins

#creating categorical variable "crime"
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

#creating table "crime"
table(crime)

#removing original variable "crim" from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

#adding new variable to scaled dataset
boston_scaled <- data.frame(boston_scaled, crime)

#number of rows in the "Boston" dataset
n <- nrow(boston_scaled)

#choosing 80% of the rows
ind <- sample(n, size = n * 0.8)

#creating the train-set
train <- boston_scaled[ind,]

#creating test-set
test <- boston_scaled[-ind,]

#saving the correct classes from test-data
correct_classes <- test$crime

#removing the variable "crime" from the test-data
test <- dplyr::select(test, -crime)

```

####The Linear discriminant Analysis (LDA)####


```{r LDA & plot}

#fitting the lda on the train-set
lda.fit <- lda(crime~., data = train)

#printing the lda.fit-object
lda.fit

#the function for the lda-biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0,
         x1 = myscale * heads[,choices[1]],
         y1 = myscale * heads[,choices[2]], col = color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads),
       cex = tex, col = color, pos = 3)
}

#targeting classes as numeric
classes <- as.numeric(train$crime)

#plotting the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

####LDA-predictions####

```{r LDA predictions}

#predicting classes with test-data
lda.pred <- predict(lda.fit, newdata = test)

#cross-tabulating the results
table(correct = correct_classes, predicted = lda.pred$class)

```

####Re-Loading "Boston" to apply k-means####

```{r reloading "Boston"}

#load the data
data("Boston")

```

```{r restandardizing "Boston"}

#center and standardize "Boston"
boston_scaled_2 <- scale(Boston)


#changing the object to a dataframe
boston_scaled_2 <- as.data.frame(boston_scaled_2)

```



```{r k-means of Boston}

set.seed(123)

#determining the number of clusters
k_max <- 10

#calculating the total within the sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

#visualizing results
qplot(x = 1:k_max, y = twcss, geom = 'line')

#k-means clustering
km <- kmeans(Boston, centers = 2)

#plotting "Boston" with clusters
pairs(Boston, col = km$cluster)

```


