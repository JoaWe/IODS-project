#**Clustering and Classification**#

This exercise is devoted to the themes of *classification* and *clustering*.

*Classification* implies that the individual elements of a target population are categorized with the aid of specific common observations that are known to the researcher in advance. The *classification model* is trained based on the respective data. The aim is to potenially add new observations to the respective classes.

*Clusters* are combined elements of the target population, which cannot be precategorized by the researcher, neither by quantity (the number of elements belonging to each group), nor by quality (meaning the common observations). The aim of cluster-analyses is to find the commonalities by which the individual elements can then be potentially categorized.

This analysis is therefore devided into two main parts:

1. The analysis of classes by means of the *linear discriminant analysis* (LDA), and
2. the analysis of clustering by means of the *distance measures*, as well as the *k-means*, as one of the more common clustering algorithms.

**The analysis will contain the following steps:**

1. The dataset "Boston" will be briefly introduced, by means of r-functions, as well as, graphical visualization.
2. The dataset will be standardized, by means of scaling.
3. A categorical variable - named "crime" - will be established, with its aid train- and test-sets will be created.
4. A linear discriminant analysis (LDA) will be attempted, and hopefully visualized by means of an LDA (bi)plot on the train set.
5. Predictions based on the former LDA will be attempted.
6. Finally, the Boston-dataset will be reloaded and standardized, the distances will be calculated, and the k-means algorithm will be applied to the dataset.

##Data at a glance##

###Loading and exploring the data *Boston*:###


```{r loading and exploring Boston}


#activating libraries
library(tidyr)
library(corrplot)
library(ggplot2)
library(dplyr)

#access MASS-Package
library(MASS)

#load the data
data("Boston")

#exploring the data
dim(Boston)
str(Boston)
summary(Boston)

```

The dataset consists of 14 variables including 506 observations relating to the city of Boston.

Here is a list of the variables and their respective meanings:

- **crim:** per capita crime rate by town.
- **zn:** proportion of residential land zoned for lots over 25,000 sq.ft.
- **indus:** proportion of non-retail business acres per town.
- **chas:** Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- **nox:** nitrogen oxides concentration (parts per 10 million).
- **rm:** average number of rooms per dwelling.
- **age:** proportion of owner-occupied units built prior to 1940.
- **dis:** weighted mean of distances to five Boston employment centres.
- **rad:** index of accessibility to radial highways.
- **tax:** full-value property-tax rate per \$10,000.
- **ptratio:** pupil-teacher ratio by town.
- **black:** 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- **lstat:** lower status of the population (percent).
- **medv:** median value of owner-occupied homes in \$1000s.

*These definitions and further information can be looked up from [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).*


The variable **crim** will be of particular importance in the next steps that are devoted to the issue of classification in this exercise.


####Graphical Overview of the dataset:####

```{r graphical overview}

#calculating the correlation-matrix & rounding
cor_matrix <- cor(Boston) %>% round(digits = 2)

#printing the correlation-matrix
cor_matrix

#visualization of the correlation-matrix
corrplot(cor_matrix, method="pie", type="lower", cl.pos="b", tl.pos="d", tl.cex=0.6)

```

The positive correlations are displayed in blue, while the negative correlations are displayed in red. The respective color-shade, as well as, the size of the "pie-piece" are displayed proportionally to the correlation-coefficients.
*--> You can get information on how to change certain aspects of the corrplot, as well as, tricks with regard to their interpretation from [here](www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram).*

This implies that of particular interest are the negative correlations between
**dis** with the variables **indus**, **nox**, and **age**. On the other hand, outstanding appears to be the positive correlation between **tax** and **rad**.
The least significant correlations appear to be with the variable **chas**.


```{r variable summaries}

summary(Boston$crim)
summary(Boston$zn)
summary(Boston$indus)
summary(Boston$chas)
summary(Boston$nox)
summary(Boston$rm)
summary(Boston$age)
summary(Boston$dis)
summary(Boston$rad)
summary(Boston$tax)
summary(Boston$ptratio)
summary(Boston$black)
summary(Boston$lstat)
summary(Boston$medv)


```


##Preparing the dataset##

####Scaling the dataset "Boston"####

To fit the LDA-model to the dataset two specific assumptions must be fulfilled:

1. On condition of the classes, the variables must be normally distributed.
2. The variance of each variable must be the same.

In order, to achieve these the dataset "Boston" will now be standardized:

```{r standardizing Boston}

#center and standardize "Boston" & changing the object to a dataframe
boston_scaled <- as.data.frame(scale(Boston))

#summarizing boston scaled
summary(boston_scaled)

```


####Creating Variable "crime"####

In the following steps I intend to create a new categorical variable, named **crime**, and use it to replace the original variable **crim**.
The quantiles are used as break points for this categorical variable. Finally, the new dataset will be divided to train- and test-sets, so that 80% of the data belongs to the train-set.

This new variable contains the categories *low*, *med_low*, *med_high*, and *high*.

**This step is necessary, because the target-variable in an LDA must be categorical!**

```{r creating variable crime}

#summary of scaled variable "crim"
summary(boston_scaled$crim)

#creating and printing the quantile vector of "crim"
bins <- quantile(boston_scaled$crim)

#printing quantile vector
bins

#creating categorical variable "crime"
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

#creating table "crime"
table(crime)

#removing original variable "crim" from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

#adding new variable to scaled dataset
boston_scaled <- data.frame(boston_scaled, crime)

#number of rows in the "Boston" dataset
n <- nrow(boston_scaled)

#choosing 80% of the rows
ind <- sample(n, size = n * 0.8)

#creating the train-set
train <- boston_scaled[ind,]

#creating test-set
test <- boston_scaled[-ind,]

#saving the correct classes from test-data
correct_classes <- test$crime

#removing the variable "crime" from the test-data
test <- dplyr::select(test, -crime)

```

##Classification-Instrument: The Linear discriminant Analysis (LDA)##

LDA is used for three mainpurposes:

1. the finding of variables that separate the data,
2. predicting the classes of new data, and
3. reducing dimensions (though this is not undertaken here).

The LDA will in this part of the exercise be fitted on the train-set. The categorical variable **crime** will serve as the target-variable, while the remaining variables of the dataset are used as predictor-variables.

```{r LDA}

#fitting the lda on the train-set
lda.fit <- lda(crime~., data = train)

#printing the lda.fit-object
lda.fit

```

The table above displays the LDA-function outcomes. As can be seen there, the LDA results in four classes (*low*, *med_low*, *med_high*, and *high*). These classes correspond to the four categories, which have been established with the variable "crime".
The remaining 13 variables are used as the predictor-variables. 
As the target-variable "crime" resulted in the four above mentioned classes, the produced coefficients of the LDA resulted in a number of three classes (4 classes - 1 = 3; LD1, LD2, and LD3) for each predictor-variable.
As the *proportion of trace*, which signifies the *between-group-variance*, finally, demonstrates that LD1 explains ca. 95% - 96% of the between-group-variance. 

*Something that I find very interesting - and I am not sure, if it points to me having made a mistake - is that everytime I knit this document anew, the values displayed in the "proportion of trace" appears to be changing a little. I am sure, I have not changed any potentially significant codes, though.*

```{r LDA biplot}

#the function for the lda-biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0,
         x1 = myscale * heads[,choices[1]],
         y1 = myscale * heads[,choices[2]], col = color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads),
       cex = tex, col = color, pos = 3)
}

#targeting classes as numeric
classes <- as.numeric(train$crime)

#plotting the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

The LDA-biplot displays the four classes in the following colours:

- black = low
- red = med_low
- green = med_high
- blue = high.

An obviously interesting aspect is that the class "high" is strongly separated from the others (with a few exceptions of "med_high").

The predictor variables are displayed as the orange arrows and they are based on the coefficients displayed in the table shown above the scatterplot. As the length and directions of these arrows displays, how each variable impacts on the LDA-model. Which makes the predictor-variable **rad**, as the one that sticks out from the others (both in direction and length of the arrow) particularly interesting. It is srongly directed towards the *cloud* of "high".


####LDA-predictions####

LDA can serve as a means of classifying new observations. Based on the priorly used LDA-model, the probabilities for the new observations - those 20% of rows earlier sorted to the test-set - belonging to which class can be calculated.

Each of these observations is categorized by belonging to the class in accordance with the highest probability.

```{r LDA predictions}

#predicting classes with test-data
lda.pred <- predict(lda.fit, newdata = test)

#cross-tabulating the results
table(correct = correct_classes, predicted = lda.pred$class)

```

##Clustering-Instruments: Distance Measures and K-Means##

####Re-Loading "Boston"####

```{r reloading Boston}

#load the data
data("Boston")

```

```{r restandardizing Boston}

#center and standardize "Boston" & changing the object to a dataframe
boston_scaled_2 <- as.data.frame(scale(Boston))

```

####Calculating distances####

Distance measuring affects the results of cluster-analyses and is therefore particularly relevant for the the continuation of the analysis-process. Various approaches for measuring the distances are in use (e.g. euklidean distance, Manhattan or Taxicab distance, Jaccard-index, or the Hamming distance). Depending on the issues and dataset one can be more appropriate or promising for the analysis than another.
As it was undertaken in the Datacamp-exercise, I will use the euclidean distance matrix (as it is generally the most common one) and the manhatten distance.

```{r calculating distances between observations}

#eucledian distance matrix
dist_eu <- dist(Boston)

#summarizing the euclidean distances
summary(dist_eu)

#manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

#summarizing manhattan-distances
summary(dist_man)

```

These attempts show that the distances between the supposed clusters vary strongly according to the distance measure used.

#### Applying k-means####

K-means is a clustering algorithm that is commonly - though not exclusively - and supposedly easily used for the analyses of clusters. A problem is, however, that small changes in a dataset can result in significantly differing outcomes.

```{r k-means of Boston}

set.seed(123)

#determining the number of clusters
k_max <- 10

#calculating the total within the sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

#visualizing results
qplot(x = 1:k_max, y = twcss, geom = 'line')

#k-means clustering
km <- kmeans(Boston, centers = 2)

#plotting "Boston" with clusters
pairs(Boston, col = km$cluster)

```


